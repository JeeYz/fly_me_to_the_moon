import os
import streamlit as st
from dotenv import load_dotenv

from langchain_ollama import ChatOllama
from langchain.schema import HumanMessage, AIMessage

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Ollama ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "llm_model" not in st.session_state:
    st.session_state.llm_model = "gemma3:4b"

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ¤– LLM ì±—ë´‡ ì„¤ì •")
    
    # ëª¨ë¸ ì„ íƒ
    st.subheader("ëª¨ë¸ ì„¤ì •")
    model_option = st.selectbox(
        "ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
        options=[
            "gemma3:4b",
            "llama3.2:3b", 
            "llama3.1", 
        ],
        index=0
    )
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
    st.subheader("ëª¨ë¸ íŒŒë¼ë¯¸í„°")
    temperature = st.slider(
        "Temperature", 
        min_value=0.0, 
        max_value=2.0, 
        value=0.7, 
        step=0.1
    )

    max_tokens = st.slider(
        "ìµœëŒ€ í† í° ìˆ˜", 
        min_value=100, 
        max_value=4000, 
        value=1000, 
        step=100
    )
    
    # ëª¨ë¸ ì ìš© ë²„íŠ¼
    if st.button("ì„¤ì • ì ìš©"):
        st.session_state.llm_model = model_option
        st.success(f"ëª¨ë¸ì´ {model_option}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.success("ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    st.divider()
    st.caption("Â© 2025 LLM ì±—ë´‡ | ëª¨ë“  ê¶Œë¦¬ ë³´ìœ ")

# ë©”ì¸ í™”ë©´ ì„¤ì •
st.title("ğŸ¤– LLM ì±—ë´‡")
st.subheader(f"í˜„ì¬ ëª¨ë¸: {st.session_state.llm_model}")

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.write(message.content)
    else:  # AIMessage
        with st.chat_message("assistant"):
            st.write(message.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.write(prompt)
    
    # AI ì‘ë‹µ ìƒì„± ì¤‘ í‘œì‹œ
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("ìƒê° ì¤‘...")
        
        try:
            # LLM ëª¨ë¸ ì´ˆê¸°í™”
            llm = ChatOllama(
                model=st.session_state.llm_model,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # ì‘ë‹µ ìƒì„±
            response = llm.invoke(st.session_state.messages)
            
            # ì‘ë‹µ í‘œì‹œ
            message_placeholder.markdown(response.content)
            
            # ì‘ë‹µ ì €ì¥
            st.session_state.messages.append(AIMessage(content=response.content))
            
        except Exception as e:
            message_placeholder.markdown(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì•± ì‹¤í–‰ ë°©ë²• ì•ˆë‚´
st.sidebar.markdown("""\n### ì‹¤í–‰ ë°©ë²•\n```bash\nstreamlit run app.py\n```""")
